\documentclass[../ana2.tex]{subfiles}
\begin{document}
\setcounter{section}{9}
\section{Die partielle Ableitung}
Situation: \( U \subset \R^n \) offen. 
\[ \abb{f}{U}{\R^m}, n,m\in\N, x\in U. \]
Frage: Was ist die Ableitung?\\
\( U \) offen: \( \forall x\in U, v\in \R^n \) 
ist \( x + tv \in U \), sofern 
\( \abs{t} \) klein genug ist.

Z. B.: \( U = I = (a,b) \subset \R = \R^1 \)
offen. \[ \abb{f}{(a,b)}{\R^m} \]
\[ f(t) \in \R^m \;\forall t\in (a,b) \]
Differenzenquotienten 
\[ f'(t_0) := \limesx{t}{t_0} 
\frac{f(t) - f(t_0)}{t - t_0}, \]
falls der Grenzwert existiert.
\[ =: \ddx{t} f(t_0) = \frac{df}{dt} (t_0). \]

Exkurs für Physiker:
\( f(t) = \) Position eines Teilchens in \( \R^3 \) \\
\( f'(t) =  \) Geschwindigkeit eines Teilchens in \( \R^3 \) \\
\( f''(t) =  \) Beschleunigung eines Teilchens in \( \R^3 \) \\

2 Teilchen:\\ 
\( f_1(t) = \) Position des ersten Teilchens in \( \R^3 \) \\
\( f_2(t) = \) Position des zweiten Teilchens in \( \R^3 \)
\[ f_1(t) = \left( \begin{array}{c}
    x_1(t)\\
    y_1(t)\\
    z_1(t)
\end{array} \right), 
f_2(t) = \left( \begin{array}{c}
    x_2(t)\\
    y_2(t)\\
    z_2(t)
\end{array} \right). \]
\[ f(t) := \begin{array}{c}
    f_1(t)\\f_2(t)
\end{array} \in \R^3 \times \R^3 = \R^6. \]
\( N \) Teilchen: \( f_j(t) = \) Position 
des \( j \)-ten Teilchens in \( \R^3 \).

\[ f(t) = \left( \begin{array}{c}
    f_1(t)\\
    \vdots \\
    f_N(t)
\end{array} \right) \in \R^3 \times \cdots \times \R^3 
= \R^{3N} \]
Statistiche Mechanik: \( N \rightarrow \infty \).

\begin{lem}
    Sei \( \abb{f}{(a,b)}{\R^m}, \) 
    \[f(t) = \begin{array}{c}
        f_1(t)\\
        \vdots \\
        f_m(t)
    \end{array}. \]
    \( \abb{f_j}{(a,b)}{\R}, t_0 \in (a,b) \).

    \[ \text{Es existiert } f'(t_0) = \limesx{t}{t_0} \frac{f(t) - f(t_0)}{t - t_0} 
    \text{ (Grenzwert in \( \R^3 \))} \]
    \[ \Leftrightarrow \abb{f_j}{(a,b)}{\R} 
    \text{ ist in \(t_0\) differenzierbar} \forall j=1,\ldots,m. \]
\end{lem}
\begin{bew}
    Selbst nachdenken.
\end{bew}
Was sollte man machen, falls \( \abb{f}{U}{\R^m}, 
U \subset \R^n \) offen, \( x\in U, v \in \R^n \)
\[ \Rightarrow \exists \delta = \delta(x,v) > 0: 
x + tv \in U \;\forall t \in (-\delta, \delta). \]
Definiere 
\[ h(t) := f(x + tv), \abb{h}{(-\delta,\delta)}{\R^m} \]
Können fragen, ob \( h \) in 
\( t = 0 \) differenzierbar ist.
\[ h'(0) = \limesx{t}{0} \frac{h(t) - h(0)}{t} \text{ sofern 
Grenzwert existiert.} \]
\[ := \ddx{t} h(0) =: \ddx{t} h(t)\vert_{t=0}. \]

\begin{defi}[Richtungsableitung]
    Sei \( \abb{f}{U}{\R^m}, U \subset \R^n \) offen, 
    \( x \in U, v \in \R^n \).
    \[ D_v f(x) := \limesx{t}{0} \frac{f(x + tv) - f(x)}{t} 
    =: \ddx{t} f(x + tv) \]
    heißt die Richtungsableitung von \( f \) in \( x\in U \), 
    in Richtung \( v \in \R^n \).
\end{defi}
\begin{bem}
    Manchmal sinnvoll
    \[ D_v f(x) = \limesx{t}{0+} \frac{f(x + tv) - f(x)}{t}. \]
\end{bem}
\begin{bsp}
    \( v = e_j \) sind kanonische Basisvektoren von \( \R^n \).
\end{bsp}
\begin{defi}[Partielle Ableitungen]
    \( \abb{f}{U}{\R^m}, U \subset \R^n \) offen, \( x\in U \).
    Die \(j\)-te partielle Ableitung von \(f\) in \(x \in U\) 
    ist gegeben durch
    \[ \partial_j f(x) := \ddxpartial{x_j} f(x) 
    := D_{e_j} f(x) = \ddx{t} f(x + t e_j) \vert_{t=0}. \]
\end{defi}
\begin{bem}
    \[ f(x) = f(x_1, \ldots, x_j, \ldots, x_n) \]
    \[ f(x+t e_j) = f(x_1, \ldots, x_{j-1}, 
    x_j + t, x_{j+1} \ldots, x_n) \]
\end{bem}
Gute Nachricht!
\begin{lem}
    Sei \( U \subset \R^n \) offen, \( x\in U \), 
    \( \abb{f,g}{U}{\R^m} \) und 
    \( \partial_j f(x), \partial_j g(x) \) existiere.
    \begin{enumerate}
        \item Linearität \( \forall \alpha, \beta \in \R \)
        \[ \partial_j (\alpha f + \beta g)(x) 
        = \alpha \partial_j f(x) + \beta \partial_j g(x). \]
        \item \[ \partial_j f(x) = \left(\begin{array}{c}
            \partial_j f_1(x) \\
            \vdots \\
            \partial_j f_m(x) \\
        \end{array}\right), 
        f = \left( \begin{array}{c}
            f_1\\
            \vdots \\
            f_m           
        \end{array} \right) \]
        Sofern eine der beiden Seiten existiert.
        \item Quotientenregel (für \( g(x) \neq 0 \))
        \[ \partial_j \left(\frac{f}{g}\right)(x) 
        = \frac{g(x) \partial_j f(x) - f(x) \partial_j g(x)}{g(x)^2} \]
        \item Kettenregel (für \( \abb{f}{U}{I \subset \R}, 
        I \) offenes Intervall, 
        \( \abb{\varphi}{I}{\R} \) differenzierbar)
        \[ \partial_j (\varphi \circ f)(x) 
        = \varphi'(f(x)) \partial(f(x)) \]
        \item Produktregel: \( \abb{f}{U}{\R^m}, 
        \abb{g}{U}{\R} \), partielle Ableitung 
        existiert in \( x_0 \in U \).
        \[ \Rightarrow \partial_j(fg)(x_0) 
        = (\partial_j f)(x_0) g(x_0) + f(x_0) \partial_j g(x_0). \]
    \end{enumerate}
\end{lem}
Schlechte Nachricht!\\
Es gibt Funktionen \( \abb{f}{U}{\R}, U \subset \R^n \) offen, 
die in \( x\in U \) alle partiellen Ableitungen haben, 
aber nicht in \( x \) stetig sind.
\begin{bsp}
    \( U = \R^2, \abb{f}{\R^2}{\R} \)
    \[ f(x,y) = \begin{cases}
        \frac{xy}{x^2 + y^2}, &(x,y) \neq (0,0)\\
        0, &(x,y) = (0,0)
    \end{cases}. \]

    \[ \partial_1 f(0,0) \partial_x f(0,0) 
    = \limesx{t}{0} \frac{f(t,0) - f(0,0)}{t} 
    = \limesx{t}{0} \frac{0 - 0}{t} = 0 \]
    genauso \( \partial_2 f(0,0) = \partial_y f(0,0) = 0 \).
    Aber \(f\) ist nicht stetig in \( (0,0) \).

    Z. B. \( f(x,0) = 0 \forall x \in \R \)
    \[ \Rightarrow \limes{n} f(\frac{1}{n},0) = 0 \]
    \( x=y, x \neq 0: f(x,x) = \frac{x^2}{x^2 + x^2} = \frac{1}{2} \) \\
    \( \limes{n} f(\frac{1}{n}, \frac{1}{n}) = \frac{1}{2} \neq 0 \) \\
    \( f \) ist nicht stetig in \( (0,0) \).
\end{bsp}
Wie rechnet man mit partiellen Ableitungen?
Z. B. \[ \abb{r}{\R^n}{\R}, x \mapsto r(x) = \abs{x} 
= \left( \sum_{l=1}^n x_l^2 \right)^{\frac{1}{2}} \]
Ist \( x \neq 0 \), dann existieren alle partiellen Ableitungen.

\[ \partial_j r(x) = \ddxpartial{x_j} 
\left( \sum_{l=0}^n x_l^2 \right)^{\frac{1}{2}} 
= \frac{1}{2} \left( \sum_{l=1}^n x_l^2 \right)^{-\frac{1}{2}} 
\cdot \ddxpartial{x_j} \underbrace{\sum_{l=1}^n x_j^2}_{=2x_j} \]
\[ = \frac{x_j}{r(x)} = \frac{x_j}{\abs{x}} \]
Quotientenregel: \( x \neq 0 \)
\( \Rightarrow \partial_j r(x) \) nach \( x_i \) 
differenzierbar.
\[ \Rightarrow \partial_j d_j r(x) 
= \frac{ \partial_i(x_j) r(x) = x_j \partial_i r(x) }{r(x)^2} 
= \frac{\partial_{ij} \abs{x} - x_k \frac{x_i}{\abs{x}}}{\abs{x}^2} \]
\[ = \frac{1}{\abs{x}} \left( \partial_{ij} - \frac{x_i x_j}{\abs{x}^2} \right). \]

Sei \( \abb{\varphi}{(0,\infty)}{\R} \) zwei Mal differenzierbar. 

\[ f = \varphi \circ r, f(x) = \varphi(r(x)) = \varphi(\abs{x}) \]
\[ \partial_j f(x) = \varphi'(r(x)) \partial_j r(x) \]
\[ = \varphi'(r(x)) \cdot \frac{x_j}{r(x)} 
= \varphi'(\abs{x}) \frac{x_j}{\abs{x}}. \]
\[ \partial_i \partial_j f(x) = \partial_i(\varphi'(r(x))) 
\frac{x_j}{\abs{x}} + \varphi'(r(x)) \partial_i \frac{x_j}{r(x)} \]
\[ = \varphi''(r(x)) \frac{x_i}{\abs{x}} \frac{x_j}{\abs{x}} 
+ \varphi'(\abs{x}) \frac{1}{\abs{x}} (\partial_{ij} - \frac{x_i x_j}{\abs{x}^2}) \]
Laplace Operator:
\[ \Delta f = \sum_{j=1}^n \frac{\partial_j^2}{\partial_j^2 x_j^2} f \]
\( \Rightarrow \) ist \(f\) rotationssymmetrisch, so ist 
\[ f(x) = \varphi(r(x)). \]
\[ \Rightarrow \Delta f(x) = \Delta(\varphi(\abs{x})) 
= \Delta \varphi(r(x)) = \sum_{j=1}^n 
\partial_j \partial_j f(x) \]
\[ = \sum_{j=1}^n (\varphi^n(\abs{x}) \frac{x_j^2}{\abs{x}^2} 
- \varphi'(\abs{x}) \frac{1}{\abs{x}} (1 - \frac{x_j^2}{\abs{x}^2}) ) \]
\[ = \varphi''(\abs{x}) - \varphi'(\abs{x}) \frac{n}{\abs{x}} 
+ \varphi'(\abs{x}) \frac{1}{\abs{x}} \]
\[ = \varphi''(\abs{x}) - \frac{n-1}{\abs{x}} \varphi'(\abs{x}) \]

% Vorlesung vom 06.06.2019
\( f(x) = \abs{x} \)
\[ \partial_i \partial_j \abs{x} 
= \frac{1}{\abs{x}} (\delta_{ij} - \frac{x_i x_j}{\abs{x}^2}) \]
\[ \abb{\varphi}{(0,\infty)}{\R} \in C^2((0,\infty), \R) \]
\[ f(x) = \varphi(r(x)) = \varphi(\abs{x}) \]


\[ \Rightarrow \partial_i \partial_j f(x) 
= \varphi''(\abs{x}) \frac{x_i x_j}{abs{x}^2}
+ \frac{\varphi'(\abs{x})}{\abs{x}} (\delta_{ij} 
- \frac{x_i x_j}{\abs{x}^2}) \]
\( \Delta = \) Laplace \( = \sum_{j=1}^d \delta_j^2 \)
\[ \Rightarrow \Delta f(x) 
= \varphi''(\abs{x}) + \varphi(\abs{x}) 
\frac{(d-1)}{\abs{x}} \]
z. B. \( f \) harmmonisch: \( \Delta f = 0
= \varphi''(\abs{x}) + \frac{n-1}{\abs{x}} \varphi'(\abs{x}) \)
\[ \Rightarrow \partial_r^2 \varphi + \frac{d-1}{r} \partial_r \varphi = 0 \]
\[ \abb{\varphi}{(0,\infty)}{\R}, r \mapsto \varphi(r) \]
\begin{align*}
    0 = \partial_r^2 \varphi + \frac{n-1}{r} \partial_r \varphi \\
    = r^{1-n} ( r^{n-1}\varphi')'
\end{align*}
\[ \Rightarrow (r^{n-1}\varphi')' = 0 \]
\[ \Rightarrow r^{n-1} \varphi' = \tilde{a}, 
\varphi' = \tilde{a} r^{1-n} \]
\[ \Rightarrow \varphi =
\begin{cases}
    a r^{2-n} + b, &n \geq 3\\
    a \ln r + b, & n = 2.
\end{cases} \]
z. B.: \( n=3: \varphi(r) = \frac{a}{r} (+b), a,b\in \R \)

Erinnerung aus Lineare Algebra:
\begin{align*}
    \mathscr{L}(\R^n, \R^m) 
    &:= \set{\text{lin. Abb. von } \R^n \text{ nach } \R^m}\\
    &= \set{\text{stetige lin. Abb. von } \R^n \text{ nach } \R^m}
\end{align*}
allgemein \( X, Y \) normierte Vektorräume
\[ \mathscr{L}(X, Y) := 
\set{\text{stetige lin. Abb. von } X \text{ nach } Y}  \]
\( \abb{A}{X}{Y} \) linear, ist stetig 
\( \Leftrightarrow A \) ist stetig in \(0\) \\
\( \Leftrightarrow A \) ist lokal gleichmäßig stetig \\
\( \Leftrightarrow A \) ist beschränkt
\( \abb{A}{X}{Y} \) linear ist beschränkt, falls 
\[ \underset{\substack{x\in X\\x \neq 0}}{\sup}
\frac{ \norm{Ax}y }{\norm{x}x} < \infty \]
\[ = \underset{\substack{x\in X\\x \neq 0}}{\sup} 
\norm{ A\left(\frac{x}{\norm{x}_X}\right) }_Y
= \underset{\substack{x\in X\\\norm{x}_X = 1}}{\sup} 
\norm{Ax}_Y \]
Man überlege sich: Ist \( \abb{A}{R^n}{\R^m} \)
linear, so ist
\[ \underset{\substack{x\in \R^n \\ \norm{x} = 1}}{\sup} \abs{Ax} < \infty \]
z.B. \( \abb{A}{\R^n}{Y} \) linear, \(x = \sum_{j=1}^n x_j e_j\)
\begin{align*}
    \norm{Ax}_Y &= \norm{\sum_{j=1}^n x_jAe_j}_Y \\
    &\leq \sum_{j=1}^n \norm{ x_j A e_j }_Y \\
    &= \sum_{j=1}^n \abs{x_j} \norm{ A e_j }_Y \\
    &\leq \underset{j=1,\ldots,n}{\max} \abs{x_j} \sum_{j=1}^n \norm{A e_j}_Y\\
    &\leq C \abs{x} < \infty
\end{align*}
\[ \Rightarrow \underset{x\in\R^n}{\sup} 
\frac{ \norm{A x}_y }{\abs{x}} \leq C < \infty. \]
\( \abb{f}{(a,b)}{\R} \) ist differenzierbar in \( X_0 \in (a,b)\)
\[ \limesx{x}{x_0} \frac{f(x)-f(x_0)}{x-x_0} = f'(x_0) \]
Taylor: \[ f(x) = f(x_0) + f'(x_0)(x - x_0) + \ldots \]
\begin{defi}[Ableitung]
    \( U \subset \R^n \) offen. Eine Funktion \(\abb{f}{U}{R^m}\)
    heißt differenzierbar in \(x_0 \in U\), falls ein
    \( A \in \mathscr{L}(\R^n, \R^m) \) existiert mit
    \[ \limesx{x}{x_0} \frac{f(x) -f(x_0) - A(x-x_0)}{\abs{x-x_0}} 
    = 0 \in \R^m. \]
    Mit der Substitution \( x = x_0 + h \), 
    erhält man die äquivalente Form 
    \[ \limesx{h}{0} \frac{ f(x_0 + h) - f(x_0) - A(h) }{\abs{h}} = 0 \]
\end{defi}
\begin{bem}
    \begin{enumerate}
        \item \( \abs{h} = \left( \sum_{j=1}^n \abs{h_j}^2 \right)^{1/2} \) 
        euklidische Norm. Man kann jede Norm auf \( \R^n \) oder 
        \( \R^m \) nehmen.
        \item \( \R^n, \R^m \), kann durch \(X, Y\) 
        normierter Vektorraum erreicht werden. \\
        \( U \subset X \) offen, \( x_0 \in U, \abb{f}{X}{Y} \) ist
        differenzierbar in \(x_0\), falls \(A \in \mathscr{L}(X, Y) \)
        existiert mit
        \[ \limesx{x}{x_0} \frac{f(x)-f(x_0)-A(x-x_0)}{\norm{x-x_0}_X} = 0 \]
        \[ \Leftrightarrow \limesx{x}{x_0} 
        \frac{\norm{f(x)-f(x_0)-A(x-x_0)}_Y}{\norm{x-x_0}_X} = 0 \]
    \end{enumerate}
    Warum ist die Ableitung eindeutig? 
    Richtung \( h \in \R^n, x_0 + th \in U 
    \;\forall \abs{t} \) klein genug
    \( \abb{A}{\R^n}{\R^m} \) linear, 
    \[ \frac{ f(x_0 + th) - f(x_0) - 
    \overbrace{A(th)}^{=\dot{t} A(h)} }{ \abs{th} }\]
    \[ = \frac{1}{\abs{h}} 
    \left( \frac{f(x_0 + th) - f(x_0)}{t} - A(h) \right) \]
    Also ist \( f \) in \( x_0 \) differenzierbar 
    mit der Ableitung \( A \in \mathscr{L}(\R^n, \R^m) \)    
    \[ \Rightarrow \underbrace{\limesx{t}{0} \frac{f(x_0+th) 
    - f(x_0)}{t}}_{= D_{h_0}f(x_0)} = A(h) 
    \text{ existiert.} \]
\end{bem}
\begin{notation}
    Wir nennen \( D f(x_0) := A \) die 
    Ableitung von \(f\) in \(x_0\).\\
    \( \abb{Df(x_0)}{\R^n}{\R^m} \) linear.
\end{notation}
\begin{satz}[Eindeutigkeit und Berechnung der Ableitung]
    Ist \( \abb{f}{U}{\R^m} \) in \( x_0 \in U \)
    differenzierbar, so gilt 
    \[ Df(\underbrace{x_0}_{\text{fest}})\underbrace{[h]}_{\text{linear}} 
    = D_n f(x_0) 
    := \limesx{t}{0} \frac{f(x_0 + th) - f(x_0)}{t}. \]
\end{satz}
\begin{bsp}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \( f(2) = z, \abb{f}{\C}{\C} \)
        \[ \abb{f}{\R^2}{\R^2}, f(z) = f(x+iy) := \begin{pmatrix}
            x^2-y^2 \\
            2xy
        \end{pmatrix}\]
        \[ (x+iy)^2 = x^2 - y^2 + 2ixy \]
        \[ h = \zeta + i \eta = \begin{pmatrix}
            \zeta \\
            \eta
        \end{pmatrix} \]
        \[ f(z + h) - f(z) = \begin{pmatrix}
            (x + \zeta)^2 - (y + \eta)^2\\
            2 (x+\zeta)(y + \eta)
        \end{pmatrix} - \begin{pmatrix}
            x^2 - y^2\\
            2xy
        \end{pmatrix} \]
        \[ = \begin{pmatrix}
            2x\zeta + \zeta^2 - 2y\eta - \eta^2\\
            2x\eta + 2y\eta + 2\zeta \eta
        \end{pmatrix} = \begin{pmatrix}
            2x \zeta - 2y \eta \\
            2x\eta + 2y \eta 
        \end{pmatrix} = \begin{pmatrix}
            \zeta^2 - \eta^2 \\
            2 \zeta \eta
        \end{pmatrix} \]
        \[ = \begin{pmatrix}
            2x & 2y\\
            2y & 2x
        \end{pmatrix} \begin{pmatrix}
            \zeta\\ \eta
        \end{pmatrix} + \begin{pmatrix}
            \zeta^2 - \eta^2\\
            2\zeta \eta
        \end{pmatrix} \]        
        \[ \Rightarrow \frac{\abs{f(z+h) - f(z - Ah)}}{\abs{h}}
        = \abs{h} \overset{h \rightarrow 0}{\longrightarrow} 0 \]
        und \[ \begin{pmatrix}
            2x & -2y\\
            2y & 2x
        \end{pmatrix} \begin{pmatrix}
            \zeta \\ \eta
        \end{pmatrix}
        = z \cdot h \]
        \( z = x + iy, h = \zeta + i \eta \)
    \end{enumerate}
\end{bsp}
Haben
\[ \frac{f(x) - f(x_0) - A(x-x_0)}{\abs{x-x_0}} 
\overset{x \rightarrow x_0}{\longrightarrow} 0 \]
falls \( f \) in \(x_0\) differenzierbar ist.
Definiere \( \varphi(x) := f(x)-f(x_0)-A(x-x_0), x \in U \)
\[ \Rightarrow f(x) = f(x_0) A(x - x_0) + \varphi(x) 
\;\forall x \in U. \]
\( \varphi(x) = \abs{x-x_0} \cdot 
\frac{\varphi(x)}{\abs{x-x_0}}
= \abs{x-x_0} \cdot \frac{f(x)-f(x_0)-A(x-x_0)}{\abs{x-x_0}} \) \\
\( x \neq x_0 \rightarrow 0 (x \rightarrow x_0) \)
\[ \Rightarrow \limesx{x}{x_0} f(x) = f(x_0) 
+ \limesx{x}{x_0} A(x-x_0) 
+ \limesx{x}{x_0} \varphi(x) = f(x_0) \]
\( \Rightarrow f \) differenzierbar \( \Rightarrow f \) stetig.

\begin{bsp}
    4. \( \abb{A}{\R^n}{\R^m} \) linear, 
    \( \abb{f}{\R^n}{\R^m}, x \mapsto f(x) := Ax \).

    \[ \Rightarrow f(x) - f(x_0) - A(x - x_0) 
    = A(x) - A(x_0) - A(x - x_0) = 0. \]
    \( \Rightarrow f \) ist in jedem Punkt \( x_0 \in \R^n \) 
    differenzierbar und \( Df(x_0) = A \in \mathscr{L}(\R^n, \R^m) \)
    \[ D f(x_0)[h] = Ah. \]

    \( \abb{f}{U}{\R^m}, U \subset \R^n \) offen,
    \( Df (x_0) \) existiert \( \forall x_0 \in U \)
    \[ \Rightarrow \abb{Df}{U}{\mathscr{L}(\R^n, \R^m)} \]
    d. h. \( \forall x_0 \in U: \abb{Df (x_0)}{\R^n}{\R^n} \) linear.

\end{bsp}
\begin{bsp}[5]
    \begin{enumerate}
        \item \( \mathscr{L}(\R^n, \R^m) \) ist ein reeller Vektorraum.
        \item hat auch eine natürliche Norm.
        \[ A \in \mathscr{L}(\R^n, \R^m) : \norm{A} := 
        \underset{\substack{x \in \R^n \\ x \neq 0}}{\sup} 
        \frac{\abs{Ax}}{\abs{x}} < \infty \]
        \( \Rightarrow \mathscr{L}(\R^n, \R^m) \) ist 
        normierter vollständiger Vektorraum.
        Symmetrisch, d. h. \( \scalarprod{x}{y} 
        = \sum_{j=1}^n x_j y_j \) Euklidisches Skalarprodukt
    \end{enumerate}

    \begin{align*}
        f(x + h) &= \scalarprod{x + h}{A(x+h)} = \scalarprod{x + h}{Ax + Ah}\\
        &= \scalarprod{x}{Ax + Ah} + \scalarprod{h}{Ax + Ah} \\
        &= \scalarprod{x}{Ax} + \scalarprod{x}{Ah} + \scalarprod{h}{Ax} + \scalarprod{h}{Ah}\\
        &= f(x) + 2\scalarprod{Ax}{h} + \scalarprod{h}{Ah}
    \end{align*}
    \[ \Rightarrow \abs{ f(x+h) - f(x) - \scalarprod{2Ax}{h} } 
    = \abs{ \scalarprod{h}{Ax} } 
    \oversett{CSU}{\leq} \abs{h} \abs{Ah} 
    \leq \abs{h} \norm{A} \abs{h} \]
    \[ \Rightarrow \frac{\abs{ f(x+h) - f(x) - \scalarprod{2Ax}{h} } }{\abs{h}} 
    \leq \norm{A} \abs{h} \rightarrow 0, h \rightarrow \infty. \]

    Aus Def. der Ableitung folgt 
    \[ Df(x)[h] = \scalarprod{2Ax}{h} \]
    \[ \Rightarrow Df(x) = \scalarprod{2Ax}{\cdot}. \]
\end{bsp}
\begin{bsp}[6]
    \[ \abb{f}{(a,b)}{\R^m}, (a, b) \subset \R \] 
    habe Ableitung \( f'(x), x \in (a,b) \) im Sinne von Analysis 1, 
    d. h. 
    \[ \limesx{t}{0} \frac{f(x+t) - f(x)}{t} = f'(x) \] 
    existiere. Dann ist \(f\) auch in \(x\) differenzierbar nach 
    Definition 5 und es gilt.

    \[ Df(x)[h] = f'(x) \cdot h \;\forall h \in \R. \]
    \begin{bew}
        \[ \frac{f(x+h) - f(x) - f'(x) \cdot h}{h} = \frac{f(x+h)-f(x)}{h} - f'(x)
        \overset{h \rightarrow 0}{\longrightarrow} 0 \]
        \[ \Rightarrow D f(x)[h] = f'(x)h \; \forall h \in \R \]
    \end{bew}
\end{bsp}
\subsection{Berechnen von Ableitungen}
Angenommen: \( \abb{f}{U}{\R^m}, U \subset \R^n \) offen
Sei in \( x \in U \) differenzierbar (im Sinne von Definition 5).
\[ \oversett{Satz 6}{\Rightarrow} Df(x)[h] = D_hf(x) \; \forall h \in \R^n \]
Sei \( b_1, b_2, \dots, b_n \) Basis für \( \R^n \)
\[ \Rightarrow h = \sum_{j=1}^n c_j b_j, c_1, \ldots, c_n \in \R \text{ eindeutig.} \]
\[ \begin{pmatrix}
    b_1 &\cdots &b_n
\end{pmatrix} \cdot \begin{pmatrix}
    c_1 \\
    \vdots \\
    c_n
\end{pmatrix} \]
\[ \Rightarrow Df(x)[h] = Df(x)[\sum_{j=1}^n c_j b_j]
= \sum_{j=1}^n  c_j \underbrace{Df(x)[b_j]}_{=D_{b_j}f(x)}
= \sum_{j=1}^n D_{b_j} f(x)c_j \]
\[ = (D_{b_1}f(x), \dots, D_{b_n}f(x))
\begin{pmatrix}
    c1 \\
    c2 \\
    \vdots \\
    c_n
\end{pmatrix} \]

Sei \( e_1, e_2, \dots, e_n \) Standardbasis für \(\R^n\).\\
\( h = \begin{pmatrix}
    h1 \\
    \vdots\\
    h_n
\end{pmatrix} = \sum_{j=1}^n h_je_j \) \\
\( \Rightarrow D_{e_j}f(x) = \partial_j f(x) \) partielle Ableitung.
\[ \Rightarrow Df(x)[h] = \sum_{j=1}^n D_{e_j}f(x)h_j 
= \sum_{j=1}^n \partial_j f(x)h_j
= \underbrace{(\partial_1f(x), \dots, \partial_n f(x))}_{\text{Jakobimatrix}}
\begin{pmatrix}
    h_1 \\
    \vdots \\
    h_n    
\end{pmatrix} \tag{*}\]
Ist \( \abb{f}{U}{\R^m}, f = \sum_{j=1}^m f_j \partial_j, \quad 
\partial_1, \ldots, \partial_m \) Standardbasis für \( \R^m \) \\
\( \abb{f_j}{U}{\R} \) Koordinatenfunktion
\[ f = \begin{pmatrix}
    f_1 \\
    f_2 \\
    \vdots \\
    f_m
\end{pmatrix} \]
\[ \partial_j f(x) = \begin{pmatrix}
    \partial_j f_1(x) \\
    \partial_j f_2(x) \\
    \vdots \\
    \partial_j f_m(x)
\end{pmatrix} \]
\[ \Rightarrow \begin{pmatrix}
    \partial_1 f(x) & \cdots &\partial_n f(x)
\end{pmatrix} 
= \begin{pmatrix}
    \partial_1 f_1(x) & \cdots & \partial_n f_1(x) \\
    \vdots & & \vdots \\
    \partial_1 f_m(x) & \cdots & \partial_n f_m(x)
\end{pmatrix} \]
Formel \((*)\) bleibt gültig, falls \( \abb{f}{U}{Y}, U \subset \R^n \)
offen. \( Y\) beliebige normierter Vektorraum. 

Gradient: \\
Angenommen \(\abb{f}{U}{\R}, U \subset \R^n \) offen, differenzierbar
in \(x \in U\)
\[ \Rightarrow Df(x)[h] ) (\partial_1f(x), \dots, \partial_nf(x))\begin{pmatrix}
    h_1\\
    \vdots \\
    h_n
\end{pmatrix} \]
\[ = \sum_{j=1}^n \partial_jf(x) h_j = \langle \nabla f(x), h \rangle \]
(Euklidisches Skalarprodukt)
\[ \nabla f(x) = \begin{pmatrix}
    \partial_1 f(x) \\
    \partial_2 f(x) \\
    \vdots \\
    \partial_n f(x)
\end{pmatrix} = \begin{pmatrix}
    \partial_1 f(x), \cdots, \partial_n f(x)
\end{pmatrix}^T \]

\begin{defi}
    Wir nennen \( \grad f(x) := \nabla f(x) \begin{pmatrix}
        \partial_1 f(x) \\
        \vdots \\
        \partial_n f(x)
    \end{pmatrix} \in \R^n \) den Gradienten von \(f\) in 
    \( x \in U \).

    Ist \( \nabla f(x) = 0 \in \R^n \), so heißt \(x\) 
    kritischer Punkt.\\
    Ist \( \nabla f(x) \neq 0 \), so heißt \(x\) 
    nicht kritisch.
\end{defi}
Sei \( \nabla f(x) \neq 0, h \in \R^n, \abs{h} = 1 \)
\[ \Rightarrow D_hf(x) = Df(x)[h] = \langle f(x), h \rangle 
\leq \abs{\langle \nabla f(x), h \rangle} \]
\[ \leq \abs{\nabla f(x)} \abs{h} = \abs{\nabla f(x)} \]
Wann gilt \gqq{\(=\)}?\\
\( \Leftrightarrow h \) und \( \forall f(x) \) lin. abhängig \\
\( \Rightarrow \) Richtung von \(h = \) Richtung von 
\( \nabla f(x) \)
\begin{bsp}
    \( f(x) = \langle x, Ax \rangle, A \subset \R^{n \times n} \)
    symmetrisch. \\
    \( \Rightarrow Df(x)[h] = \langle 2Ax, h \rangle
    \Rightarrow \nabla f(x) = 2Ax \)
\end{bsp}
\begin{satz}[Differenzierbar \(\Rightarrow\) stetig]
    Sei \(U \subset \R^n\) offen, \( \abb{f}{U}{\R^n} \)
    differenzierbar in \( x_0 \in U \)
    \[ \Rightarrow f \text{ ist stetig in } x_0 \]
\end{satz}
\begin{bew}
    Setzen \( \varphi(x) = f(x) - f(x_0) - A(x - x_0) \)
    \[ \Rightarrow f(x) 
    = f(x_0) + A(x - x_0) + \varphi(x) \rightarrow f(x_0), 
    x\rightarrow x_0 \]
    Sei \( x + x_0, \varphi(x) = 
    \abs{x - x_0} \frac{f(x) - f(x_0) - A(x - x_0)}{\abs{x - x_0}} \)
    da \( f \) in \(x_0\) differenzierbar und \(A = D f(x_0)\).
\end{bew}
\begin{lem}
    Sei \( U \subset \R^n\) offen, \(\abb{f}{U}{\R^m}\).
    Dann gilt:
    \(f\) ist differenzierbar in \(x_0 \in U \) mit Ableitung
    \( A = Df(x_0) \)
    \( \Leftrightarrow \exists \) Funktion, \( \varepsilon = 
    \abb{\varepsilon_{x_0}}{U}{\R^m} \), stetig in \(x_0\) und
    \(\varepsilon(x_0) = 0\) mit 
    \[ f(x) = \underbrace{f(x_0) + A(x-x_0)}_{\text{affin linear}} 
    + \underbrace{\abs{x-x_0}\varepsilon(x)}_{\text{Fehler}}
    \; \forall x \in U \]
\end{lem}
\begin{bew}
    Definiere 
    \[ \varepsilon(x) = \varepsilon_{x_0}(x) 
    := \begin{cases}
        0, & x = x_0 \\
        \frac{f(x) - f(x_0) - A(x - x_0)}{\abs{x - x_0}}, & x\in U \setminus \set{x_0}
    \end{cases} \]
    \[ \Rightarrow f(x) = f(x_0) + A(x - x_0) + \abs{x - x_0} \varepsilon(x) \]
    Dann scharf hinschauen (mit Brille am besten).
\end{bew}
Anwendung: Sei \( U \subset \R^n \) offen. \\
Linearität: \( \abb{f,g}{U}{\R^m} \) differenzierbar in \(x_0 in U\)
\( \Rightarrow \alpha f + \beta g \) ist differenzierbar in 
\(x_0 \; \forall \alpha, \beta \in \R\) \\
\( D(\alpha f + \beta f)(x_0) = \alpha Df(x_0) + \beta D_g(x_0) \)
\begin{bew}
    \( A = Df (x_0), B = Dg(x_0) \)
    Haben (Lemma 8) 
    \[ f(x) = f(x_0) + A(x - x_0) + \abs{x - x_0}\varepsilon(x), 
    \abb{\varepsilon}{U}{\R^m} \text{ stetig und } \varepsilon(x_0) = 0 \]
    \[ g(x) = g(x_0) + B(x - x_0) + \abs{x - x_0}\eta(x), 
    \abb{\eta}{U}{\R^m} \text{ stetig und } \eta(x_0) = 0 \]
    \[ (\alpha f +\beta g)(x) 
    = \alpha f(x) + \beta g(x) \]
    \[ = \alpha (f(x_0) + A(x - x_0) + \abs{x - x_0}\varepsilon(x)) 
    + \beta (g(x_0) + A(x - x_0) + \abs{x - x_0}\eta(x)) \]
    \[ \alpha f(x_0) + \beta g(x_0) + \alpha A(x-x_0)+\beta B(x-x_0) + \abs{x-x_0}
    (\alpha \varepsilon(x) + \beta \eta(x)) \]
    \( \abb{\gamma}{U}{\R} \) ist stetig in \(x_0\), 
    \( \gamma(x_0) = 0 \).
    \[ \oversett{Lemma 8}{\Rightarrow} 
    D(\alpha f + \beta g)(x) = \alpha A + \beta B. \]
\end{bew}
Produktregel: Sei \( \abb{f}{U}{R^m}, \abb{g}{U}{\R} \)
differenzierbar in \(x_0\) \\
\( \Rightarrow  fg \) differenzierbar in \(x_0\): 
\( D(fg)(x_0) = Df(x_0)g(x_0) + f(x_0)Dg(x_0) \)
\begin{bew}
    \[ (fg)(x) = f(x)g(x) 
    = (f(x_0) + A(x - x_0) + \abs{x - x_0}\varepsilon(x)) 
    (g(x_0) + B(x - x_0) + \abs{x - x_0}\eta(x))  \]
    \[ = f(x_0)g(x_0) + A(x - x_0) g)x_0 
    + f(x_0) + B(x - x_0) + \text{ Rest.} \]
    \[\mathrm{Rest}(x) = A(x-x_0)B(x-x_0) + \abs{x-x_0}
    (f(x_0)\eta(x) + \varepsilon(x)g(x_0) + A(x-x_0)\eta(x)
    + \varepsilon(x)B(x-x_0) + \abs{x-x_0} \varepsilon(x)\eta(x))\]

    \[ \abs{ A(x - x_0) B(x - x_0) } 
    = \abs{A (x- x_0)}\abs{B(x-x_0)} \]
    \[ \leq \norm{A}\abs{x-x_0} \norm{B}\abs{x - x_0}
    = \norm{A} \norm{B} \abs{x - x_0}^2 \]
    \[ \gamma(x) = \begin{cases}
        0, &x = x_0\\
        \frac{A(x - x_0)B(x - x_0)}{\abs{x - x_0}} + (**), &x \in U \setminus \set{x_0}.
    \end{cases} \]
    Ähnlich: Quotientenregel \( \abb{f}{U}{\R^m}, \abb{g}{U}{\R} \)
    \[ g(x_0) \neq 0 \]
    \[ D(\frac{f}{g})(x_0) 
    = \frac{D(f)(x_0) g(x_0) - D(g)(x_0) f(x_0)}{g(x_0)^2} \]
\end{bew}
\begin{satz}[Kettenregel]
    Seien \( \abb{f}{U}{\R^m} \), 
    \( \abb{g}{V}{\R^p} \)
    mit \( U \subset \R^n, V \subset \R^m \)
    offen und \( f(U) \subset V \).
    Sind \(f\) in \(x_0\) und \(g\) in \(y_0 = f(x_0)\)
    differenzierbar, so ist auch \(\abb{g \circ f}{U}{R^p}\)
    differenzierbar in \(x_0\) und es gilt die 
    Kettenregel 
    \[ D(g\circ f)(x_0) = D_g(f(x_0))Df(x_0) \]
    Sind \( \abb{A = D_g(y_0)}{\R^m}{\R^p} \) und 
    \[ B = \abb{Df(x_0)}{\R^n}{\R^m} \]
    die zugehörige Jacobimatrizen, so folgt 
    \[ C := AB \]
    wobei
    \[ \abb{C}{\R^n}{\R^p} \] 
    die Jacobimatrix zu \( D(f \circ g)(x_0) \) ist.
\end{satz}
\begin{bew}
    Sei \(A = D_g(y_0), y_0 = f(x_0), B = Df(x_0) \).
    \[ \oversett{Lem. 8}{\Leftrightarrow} 
    f(x_0 + h) = f(x_0) + Bh + \abs{h} \varepsilon(h), \]
    \( \varepsilon \) stetig in \(0. \varepsilon(0) = 0\).
    \[ g(x_0 + k) = g(x_0) + Ak + \abs{k} \eta(k) \]
    \( \eta \) stetig in \( 0 \), \( \eta(0) = 0 \).

    \begin{align*}
        \Rightarrow (g \circ f)(x_0 + h) &= g(f(x_0 + h))\\
        &= g(f(x_0)) + Bh + \abs{h}\varepsilon(h) \\
        &= g(f(x_0)) + A(Bh + \abs{h}\varepsilon(h)) \\
        &+ \abs{ Bh + \abs{h}\varepsilon(h) } \eta (Bh + \abs{h}\varepsilon(h)) \\
        &= g(f(x_0)) + ABh + \underbrace{\abs{h} A \varepsilon(h) + \abs{Bh \abs{h}\varepsilon(h)} \eta (Bh + \abs{h}\varepsilon(h))}_{=:\abs{h}\rho(h)}
    \end{align*}
    Beachte 
    \[ \frac{\abs{ Bh + \abs{h}\varepsilon(h) }}{\abs{h}}
    \leq B(\frac{h}{\abs{h}}) + \abs{\varepsilon(h)} 
    \leq \norm{B} + \underbrace{\abs{\varepsilon(h)}}_{\underset{h \rightarrow 0}{\longrightarrow 0}} \]
    \[ \Rightarrow \exists c < \infty: \frac{\abs{Bh + \abs{h}\varepsilon(h)}}{\abs{h}} 
    \leq C \; \forall \abs{h} \leq 1 \]
    \[ \Rightarrow \forall \abs{h} \leq 1: 
    \abs{\rho(h)} \leq | \underbrace{A(\underbrace{\varepsilon(h)}_{\rightarrow 0})}_{\rightarrow 0} 
    + C\underbrace{\rho (Bh + \abs{h}\varepsilon(h)}_{\rightarrow 0}) | \]    
    \[ \Rightarrow \limesx{h}{0} \abs{\rho(h)} = 0. \]
    \( \oversett{Lem. 8}{\Rightarrow} g \circ f \) 
    ist differenzierbar in \(x_0\) mit der Ableitung \( AB \).
\end{bew}
\end{document}